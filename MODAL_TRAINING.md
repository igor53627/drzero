# Dr. Zero Modal Training Guide

## Quick Start

```bash
# 1. Smoke test (verify environment)
modal run modal_train.py --action smoke

# 2. Prepare training data
modal run modal_train.py --action prepare-data

# 3. Run challenger training (detached)
modal run --detach modal_train.py --action challenger --iteration 1
```

## Assets Location

### Modal Volumes

| Volume | Mount Path | Contents |
|--------|------------|----------|
| `dr-zero-corpus` | `/corpus` | ChromaDB, training data, cached models |
| `dr-zero-checkpoints` | `/checkpoints` | Training checkpoints |

### Volume Contents

```
/corpus/
├── chromadb/           # ChromaDB with iO papers corpus
├── data/
│   └── zero_ratio4321.parquet  # Training data (generated by prepare-data)
└── models/
    └── Qwen3-32B/      # Cached model weights (uploaded from local)

/checkpoints/
└── challenger_iter1/   # Training checkpoints
```

### Local Files (uploaded via Image.add_local_dir)

```
/root/drzero/           # Full repo copy
├── config/
│   ├── search_multiturn_grpo.yaml
│   └── search_tool_config.yaml
├── verl/
│   └── custom_reward/
│       ├── reward_function.py
│       └── reward_rollout.py
├── search/
│   └── chromadb_server.py
└── modal_train.py
```

## Volume Management

```bash
# List volume contents
modal volume ls dr-zero-corpus
modal volume ls dr-zero-corpus /models/Qwen3-32B

# Upload model to volume (run locally first: huggingface-cli download Qwen/Qwen3-32B --local-dir ~/models/Qwen3-32B)
modal volume put dr-zero-corpus ~/models/Qwen3-32B /models/Qwen3-32B -f

# Upload ChromaDB
modal volume put dr-zero-corpus /path/to/chromadb /chromadb -f

# Download checkpoints
modal volume get dr-zero-checkpoints /challenger_iter1 ./local_checkpoints
```

## Debugging

### Check running apps
```bash
modal app list
```

### View logs (latest app)
```bash
timeout 5 modal app logs <app-id> 2>&1 | tail -50
```

### Common error patterns
```bash
# Find specific errors
timeout 5 modal app logs <app-id> 2>&1 | grep -E "ValueError|TypeError|Please set" | head -10

# Find what happened before crash
timeout 5 modal app logs <app-id> 2>&1 | grep -B5 "SIGTERM" | head -20
```

### Stop a running app
```bash
modal app stop <app-id>
```

## Configuration

### GPU Setup
- 8x H100 GPUs
- SGLang: dp_size=4, tp_size=2 (4 data parallel x 2 tensor parallel)
- mem_fraction_static=0.5

### Model Options
```bash
modal run modal_train.py --action list-models

# Available:
# qwen3-32b  -> /corpus/models/Qwen3-32B (cached)
# qwen3-14b  -> Qwen/Qwen3-14B
# qwen3-8b   -> Qwen/Qwen3-8B
```

### Key Training Parameters
| Parameter | Value | Notes |
|-----------|-------|-------|
| train_batch_size | 256 | Global batch |
| micro_batch_size_per_gpu | 2 | Per-GPU micro batch |
| tp_size | 2 | Tensor parallel |
| dp_size | 4 | Data parallel |
| max_prompt_length | 1536 | From config |
| max_response_length | 2560 | From config |

## Services Started During Training

1. **Retrieval Server** (port 8000)
   - ChromaDB-based paper retrieval
   - Uses e5-large-v2 embeddings
   - Health check: `http://127.0.0.1:8000/docs`

2. **SGLang Server** (port 8001)
   - LLM inference for reward rollouts
   - Health check: `http://127.0.0.1:8001/health`

## Common Issues

### flash-attn build takes too long / fails
- **Symptom**: Image build hangs or times out compiling `flash-attn` from source
- **Fix**: Install a prebuilt wheel that matches the base image (`torch 2.9.1+cu129`, `py312`).
  This avoids CUDA compilation and keeps the base `lmsysorg/sglang` image.

In `modal_train.py`:
```python
.run_commands(
    "python -m pip uninstall -y flash-attn flash_attn || true",
    "python -m pip install "
    "https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/"
    "flash_attn-2.8.3+cu12torch2.9cxx11abiTRUE-cp312-cp312-linux_x86_64.whl "
    "--no-deps",
)
```

### sglang API drift (tokenizer manager / utils imports)
- **Symptom**: Import errors like
  - `ReleaseMemoryOccupationReqInput` missing
  - `get_ip` / `maybe_set_triton_cache_manager` missing
- **Fix**: Use compatibility shims in `verl/workers/rollout/sglang_rollout/sglang_rollout.py`
  so the rollout code works across sglang versions in the base image.

### smoke test times out on first build
- **Symptom**: `FunctionTimeoutError` during `modal run modal_train.py --action smoke`
- **Fix**: Increase the smoke timeout to 30 minutes (first image build can be slow).

### HuggingFace Timeouts
- **Symptom**: ReadTimeoutError downloading model
- **Fix**: Pre-download model locally and upload to volume

### Missing micro_batch_size
- **Symptom**: ValueError "Please set micro_batch_size_per_gpu"
- **Fix**: Add `+actor_rollout_ref.actor.micro_batch_size_per_gpu=2` etc. (note the `+` prefix for new keys)

### Hydra Config Override
- Use `+key=value` to add new keys not in base config
- Use `key=value` to override existing keys

### Missing val_files
- **Symptom**: FileNotFoundError for test.parquet
- **Fix**: Set `data.val_files` to same as train_files

## Secrets Required

Create in Modal dashboard:
- `wandb-secret`: WANDB_API_KEY
- `huggingface-secret`: HF_TOKEN

## Cost Estimate

- 8x H100: ~$32/hour on Modal
- Full training run: 24 hours = ~$768
